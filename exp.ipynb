{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "868bd6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 88, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 88, 8)        32          ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 44, 8)       0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 44, 16)       400         ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 22, 16)      0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 352)          0           ['max_pooling1d_3[0][0]']        \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          45184       ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 2)            258         ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 2)            258         ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " sampling_1 (Sampling)          (None, 2)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 46,132\n",
      "Trainable params: 46,132\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               384       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 704)               90816     \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 88, 8)             0         \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 88, 16)            400       \n",
      "                                                                 \n",
      " up_sampling1d_2 (UpSampling  (None, 176, 16)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, 176, 8)            392       \n",
      "                                                                 \n",
      " up_sampling1d_3 (UpSampling  (None, 352, 8)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_9 (Conv1D)           (None, 352, 1)            25        \n",
      "                                                                 \n",
      " cropping1d_1 (Cropping1D)   (None, 88, 1)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 92,017\n",
      "Trainable params: 92,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "latent_dim = 2\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(88, 1))\n",
    "x = layers.Conv1D(8, 3, activation='relu', padding='same', dilation_rate=2)(encoder_inputs)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Conv1D(16, 3, activation='relu', padding='same', dilation_rate=2)(x)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Flatten()(x)\n",
    "encoder = layers.Dense(128, activation='relu')(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(encoder)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(encoder)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(128, activation='relu')(latent_inputs)\n",
    "x = layers.Dense(88 * 8)(x)\n",
    "x = layers.Reshape((88, 8))(x)\n",
    "x = layers.Conv1D(16, 3, activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling1D(2)(x)\n",
    "x = layers.Conv1D(8, 3, activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling1D(2)(x)\n",
    "decoded = layers.Conv1D(1, 3, activation='sigmoid', padding='same')(x)\n",
    "decoded = layers.Cropping1D(cropping=(0, 264))(decoded)  # Crop to the desired output shape\n",
    "decoder = keras.Model(latent_inputs, decoded, name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum((data - reconstruction) ** 2, axis=(1, 2))\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x = data\n",
    "\n",
    "        z_mean, z_log_var, z = self.encoder(x, training=False)\n",
    "        reconstructed_x = self.decoder(z, training=False)\n",
    "\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum((x - reconstructed_x) ** 2, axis=(1, 2))\n",
    "        )\n",
    "\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "\n",
    "        total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results[\"reconstruction_loss\"] = reconstruction_loss\n",
    "        results[\"kl_loss\"] = kl_loss\n",
    "        results[\"total_loss\"] = total_loss\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cda89322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features = pd.read_csv('features.csv')\n",
    "labels = pd.read_csv('labels.csv')\n",
    "(x_train, x_test, y_train, y_test) = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train_seq = x_train.values[:, 0]\n",
    "x_test_seq = x_test.values[:, 0]\n",
    "y_train_labels = y_train\n",
    "y_test_labels = y_test\n",
    "\n",
    "#print(x_train)\n",
    "#print(x_test)\n",
    "#print(y_train)\n",
    "#print(y_test)\n",
    "\n",
    "# Convert columns to NumPy arrays\n",
    "x_train_features = x_train.values[:, 1:]\n",
    "x_test_features = x_test.values[:, 1:]\n",
    "\n",
    "# Apply scaling to the features\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train = scaler.fit_transform(x_train_features)\n",
    "x_test = scaler.transform(x_test_features)\n",
    "\n",
    "#print(x_train)\n",
    "#print(x_test)\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "y_train = np.expand_dims(y_train, -1)\n",
    "y_test = np.expand_dims(y_test, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b578085e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "262/262 [==============================] - 4s 7ms/step - loss: 0.9758 - reconstruction_loss: 0.9664 - kl_loss: 2.6253e-05 - val_total_loss: 0.9065 - val_reconstruction_loss: 0.9065 - val_kl_loss: 7.2032e-05\n",
      "Epoch 2/2\n",
      "262/262 [==============================] - 2s 6ms/step - loss: 0.9641 - reconstruction_loss: 0.9648 - kl_loss: 7.1113e-06 - val_total_loss: 0.9336 - val_reconstruction_loss: 0.9336 - val_kl_loss: 1.0371e-05\n",
      "66/66 [==============================] - 0s 2ms/step - total_loss: 0.9339 - reconstruction_loss: 0.9338 - kl_loss: 1.0371e-05\n",
      "Epoch 1/2\n",
      "131/131 [==============================] - 3s 11ms/step - loss: 0.9579 - reconstruction_loss: 0.9626 - kl_loss: 1.1161e-05 - val_total_loss: 0.8738 - val_reconstruction_loss: 0.8738 - val_kl_loss: 3.0398e-06\n",
      "Epoch 2/2\n",
      "131/131 [==============================] - 1s 9ms/step - loss: 0.9446 - reconstruction_loss: 0.9599 - kl_loss: 5.2252e-06 - val_total_loss: 0.8632 - val_reconstruction_loss: 0.8632 - val_kl_loss: 3.2783e-06\n",
      "66/66 [==============================] - 0s 2ms/step - total_loss: 0.9062 - reconstruction_loss: 0.9062 - kl_loss: 3.2783e-06\n",
      "Epoch 1/2\n",
      "262/262 [==============================] - 4s 7ms/step - loss: 0.9722 - reconstruction_loss: 0.9748 - kl_loss: 3.3960e-05 - val_total_loss: 0.9302 - val_reconstruction_loss: 0.9302 - val_kl_loss: 2.3544e-06\n",
      "Epoch 2/2\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.9625 - reconstruction_loss: 0.9658 - kl_loss: 1.1772e-05 - val_total_loss: 0.9223 - val_reconstruction_loss: 0.9222 - val_kl_loss: 1.7315e-05\n",
      "66/66 [==============================] - 0s 2ms/step - total_loss: 0.9217 - reconstruction_loss: 0.9217 - kl_loss: 1.7315e-05\n",
      "Epoch 1/2\n",
      "131/131 [==============================] - 3s 12ms/step - loss: 0.9719 - reconstruction_loss: 0.9683 - kl_loss: 1.4146e-05 - val_total_loss: 0.8818 - val_reconstruction_loss: 0.8818 - val_kl_loss: 8.6129e-06\n",
      "Epoch 2/2\n",
      "131/131 [==============================] - 1s 10ms/step - loss: 0.9614 - reconstruction_loss: 0.9640 - kl_loss: 1.1770e-05 - val_total_loss: 0.8804 - val_reconstruction_loss: 0.8804 - val_kl_loss: 5.9605e-07\n",
      "66/66 [==============================] - 0s 2ms/step - total_loss: 0.9163 - reconstruction_loss: 0.9163 - kl_loss: 5.9605e-07\n",
      "Best Hyperparameters: {'learn_rate': 0.005, 'batch_size': 64}\n",
      "Best Score: [0.9062418341636658, 0.9062385559082031, 3.2782554626464844e-06]\n"
     ]
    }
   ],
   "source": [
    "#learn_rate = [0.005, 0.01, 0.05, 0.1]\n",
    "#batch_size = [32, 64, 128, 256]\n",
    "learn_rate = [0.005, 0.01]\n",
    "batch_size = [32, 64]\n",
    "param_grid = dict(learn_rate=learn_rate, batch_size=batch_size)\n",
    "\n",
    "best_score = None\n",
    "best_params = {}\n",
    "\n",
    "for lr in learn_rate:\n",
    "    for bs in batch_size:\n",
    "        # Create the VAE model with the current hyperparameters\n",
    "        vae = VAE(encoder, decoder)\n",
    "        vae.compile(optimizer=keras.optimizers.Adam(learning_rate=lr))\n",
    "\n",
    "        # Train the model\n",
    "        vae.fit(x_train, batch_size=bs, epochs=2, validation_data=(x_test,))\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        score = vae.evaluate(x_test)\n",
    "\n",
    "        # Update the best score and parameters if needed\n",
    "        if best_score is None or score < best_score:\n",
    "            best_score = score\n",
    "            best_params = {'learn_rate': lr, 'batch_size': bs}\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "lr = best_params['learn_rate']\n",
    "bs = best_params['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b0db432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "131/131 [==============================] - 3s 11ms/step - loss: 0.9810 - reconstruction_loss: 0.9583 - kl_loss: 1.8441e-06 - val_total_loss: 0.8815 - val_reconstruction_loss: 0.8815 - val_kl_loss: 1.3411e-06 - lr: 0.0050\n",
      "Epoch 2/20\n",
      "131/131 [==============================] - 1s 10ms/step - loss: 0.9610 - reconstruction_loss: 0.9567 - kl_loss: 1.2599e-06 - val_total_loss: 0.8808 - val_reconstruction_loss: 0.8808 - val_kl_loss: 2.5332e-06 - lr: 0.0050\n",
      "Epoch 3/20\n",
      "131/131 [==============================] - 1s 10ms/step - loss: 0.9650 - reconstruction_loss: 0.9568 - kl_loss: 8.8178e-07 - val_total_loss: 0.8733 - val_reconstruction_loss: 0.8733 - val_kl_loss: 1.1623e-06 - lr: 0.0050\n",
      "Epoch 4/20\n",
      "131/131 [==============================] - 1s 10ms/step - loss: 0.9525 - reconstruction_loss: 0.9555 - kl_loss: 4.1222e-07 - val_total_loss: 0.8736 - val_reconstruction_loss: 0.8736 - val_kl_loss: 5.9605e-08 - lr: 0.0050\n",
      "Epoch 5/20\n",
      "131/131 [==============================] - 1s 9ms/step - loss: 0.9606 - reconstruction_loss: 0.9557 - kl_loss: 5.9491e-07 - val_total_loss: 0.8697 - val_reconstruction_loss: 0.8697 - val_kl_loss: 8.9407e-08 - lr: 0.0050\n",
      "Epoch 6/20\n",
      "131/131 [==============================] - 1s 10ms/step - loss: 0.9627 - reconstruction_loss: 0.9555 - kl_loss: 2.7027e-07 - val_total_loss: 0.8737 - val_reconstruction_loss: 0.8737 - val_kl_loss: 1.4901e-07 - lr: 0.0050\n",
      "Epoch 7/20\n",
      "131/131 [==============================] - 1s 9ms/step - loss: 0.9597 - reconstruction_loss: 0.9552 - kl_loss: 3.9835e-07 - val_total_loss: 0.8632 - val_reconstruction_loss: 0.8632 - val_kl_loss: 1.7881e-07 - lr: 0.0050\n",
      "Epoch 8/20\n",
      "131/131 [==============================] - 1s 9ms/step - loss: 0.9690 - reconstruction_loss: 0.9553 - kl_loss: 4.8139e-07 - val_total_loss: 0.8634 - val_reconstruction_loss: 0.8634 - val_kl_loss: 5.9605e-08 - lr: 0.0050\n",
      "Epoch 9/20\n",
      "131/131 [==============================] - 1s 10ms/step - loss: 0.9653 - reconstruction_loss: 0.9554 - kl_loss: 1.8996e-07 - val_total_loss: 0.8699 - val_reconstruction_loss: 0.8699 - val_kl_loss: 5.9605e-08 - lr: 0.0050\n",
      "Epoch 10/20\n",
      "126/131 [===========================>..] - ETA: 0s - loss: 0.9691 - reconstruction_loss: 0.9571 - kl_loss: 2.5427e-07\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "131/131 [==============================] - 1s 9ms/step - loss: 0.9685 - reconstruction_loss: 0.9555 - kl_loss: 2.4593e-07 - val_total_loss: 0.8713 - val_reconstruction_loss: 0.8713 - val_kl_loss: 2.9802e-08 - lr: 0.0050\n",
      "Epoch 11/20\n",
      "131/131 [==============================] - 1s 10ms/step - loss: 0.9726 - reconstruction_loss: 0.9541 - kl_loss: 8.0990e-08 - val_total_loss: 0.8785 - val_reconstruction_loss: 0.8785 - val_kl_loss: 4.7684e-07 - lr: 0.0025\n",
      "Epoch 12/20\n",
      "131/131 [==============================] - 1s 9ms/step - loss: 0.9512 - reconstruction_loss: 0.9539 - kl_loss: 9.0544e-08 - val_total_loss: 0.8754 - val_reconstruction_loss: 0.8754 - val_kl_loss: 0.0000e+00 - lr: 0.0025\n",
      "Epoch 13/20\n",
      "131/131 [==============================] - 1s 9ms/step - loss: 0.9522 - reconstruction_loss: 0.9536 - kl_loss: 3.8675e-08 - val_total_loss: 0.8742 - val_reconstruction_loss: 0.8742 - val_kl_loss: 0.0000e+00 - lr: 0.0025\n",
      "Epoch 14/20\n",
      "131/131 [==============================] - 1s 9ms/step - loss: 0.9627 - reconstruction_loss: 0.9534 - kl_loss: 2.5707e-08 - val_total_loss: 0.8744 - val_reconstruction_loss: 0.8744 - val_kl_loss: 1.1921e-07 - lr: 0.0025\n",
      "Epoch 15/20\n",
      "131/131 [==============================] - 1s 9ms/step - loss: 0.9592 - reconstruction_loss: 0.9530 - kl_loss: 2.9097e-07 - val_total_loss: 0.8699 - val_reconstruction_loss: 0.8699 - val_kl_loss: 2.9802e-08 - lr: 0.0025\n",
      "Epoch 16/20\n",
      "131/131 [==============================] - 1s 9ms/step - loss: 0.9448 - reconstruction_loss: 0.9533 - kl_loss: 2.6162e-08 - val_total_loss: 0.8737 - val_reconstruction_loss: 0.8737 - val_kl_loss: 0.0000e+00 - lr: 0.0025\n",
      "Epoch 17/20\n",
      "131/131 [==============================] - 1s 10ms/step - loss: 0.9501 - reconstruction_loss: 0.9525 - kl_loss: 1.0524e-06 - val_total_loss: 0.8709 - val_reconstruction_loss: 0.8709 - val_kl_loss: 8.9407e-08 - lr: 0.0025\n",
      "Epoch 18/20\n",
      "131/131 [==============================] - 1s 9ms/step - loss: 0.9356 - reconstruction_loss: 0.9541 - kl_loss: 1.2467e-07 - val_total_loss: 0.8663 - val_reconstruction_loss: 0.8663 - val_kl_loss: 5.9605e-08 - lr: 0.0025\n",
      "Epoch 19/20\n",
      "131/131 [==============================] - 1s 9ms/step - loss: 0.9419 - reconstruction_loss: 0.9531 - kl_loss: 4.2542e-08 - val_total_loss: 0.8706 - val_reconstruction_loss: 0.8706 - val_kl_loss: 0.0000e+00 - lr: 0.0025\n",
      "Epoch 20/20\n",
      "131/131 [==============================] - ETA: 0s - loss: 0.9556 - reconstruction_loss: 0.9540 - kl_loss: 1.5015e-08\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "131/131 [==============================] - 1s 9ms/step - loss: 0.9556 - reconstruction_loss: 0.9540 - kl_loss: 1.5015e-08 - val_total_loss: 0.8769 - val_reconstruction_loss: 0.8769 - val_kl_loss: 0.0000e+00 - lr: 0.0025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x222f0aa7e80>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam(learning_rate = lr))\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, verbose=1, min_lr=1e-6)\n",
    "early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "vae.fit(x_train, batch_size=bs, epochs=20, validation_data=(x_test,), callbacks=[reduce_lr, early_stop])\n",
    "#vae.fit(x_train, batch_size=128, epochs=5, validation_data=(x_test, y_test, None))\n",
    "#vae.fit(x_train, batch_size=128, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f59f016d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 0s 2ms/step - total_loss: 0.9157 - reconstruction_loss: 0.9157 - kl_loss: 0.0000e+00\n",
      "Validation Loss: [0.915713369846344, 0.915713369846344, 0.0]\n"
     ]
    }
   ],
   "source": [
    "#print(x_test.shape)\n",
    "#print(y_test.shape)\n",
    "\n",
    "validation_loss = vae.evaluate(x_test,)\n",
    "print(\"Validation Loss:\", validation_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4627b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# encode data into the latent space\n",
    "_, _, encoded_x_train = vae.encoder.predict(x_train)\n",
    "_, _, encoded_x_test = vae.encoder.predict(x_test)\n",
    "\n",
    "label_colors = {0: 'blue', 1: 'red', 2: 'green', 3: 'orange'}\n",
    "\n",
    "# input data\n",
    "plt.figure(figsize=(10, 6))\n",
    "for label in range(4):\n",
    "    indices_train = np.where(y_train == label)[0]\n",
    "    plt.scatter(encoded_x_train[indices_train, 0], encoded_x_train[indices_train, 1], c=label_colors[label],\n",
    "                label='Label {}'.format(label))\n",
    "plt.title('Latent Space Visualization - Input Data')\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# validation data\n",
    "plt.figure(figsize=(10, 6))\n",
    "for label in range(4):\n",
    "    indices_test = np.where(y_test == label)[0]\n",
    "    plt.scatter(encoded_x_test[indices_test, 0], encoded_x_test[indices_test, 1], c=label_colors[label],\n",
    "                label='Label {}'.format(label))\n",
    "plt.title('Latent Space Visualization - Validation Data')\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb47b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_input_and_reconstructed_data(x_input_data, y_input_data):\n",
    "    _, _, z_input = encoder.predict(x_input_data)\n",
    "    reconstructed_data = vae.decoder.predict(z_input)\n",
    "    _, _, recon_data_lsp = vae.encoder.predict(reconstructed_data)\n",
    "    \n",
    "    unique_labels = np.unique(y_input_data)\n",
    "    label_colors = ['blue', 'red', 'green', 'orange']\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        indices = np.where(y_input_data == label)[0]\n",
    "        plt.scatter(z_input[indices, 0], z_input[indices, 1], c=label_colors[label], label=f'Label {label}')\n",
    "\n",
    "    plt.scatter(recon_data_lsp[:, 0], recon_data_lsp[:, 1], c='black', alpha=0.5, label='Reconstructed Data')\n",
    "\n",
    "    plt.xlabel(\"Latent Dimension 1\")\n",
    "    plt.ylabel(\"Latent Dimension 2\")\n",
    "    plt.title(\"Input Data and Reconstructed Data in Latent Space\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming `x_test` and `y_test` are the original input data\n",
    "# and `reconstructed_x_test` is the reconstructed data obtained from the decoder\n",
    "plot_input_and_reconstructed_data(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ccb2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_input_data(encoder, x_data, y_data):\n",
    "    _, _, z_input = encoder.predict(x_data)\n",
    "    unique_labels = np.unique(y_data)\n",
    "    label_colors = ['blue', 'red', 'green', 'orange']\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        indices = np.where(y_data == label)[0]\n",
    "        plt.scatter(z_input[indices, 0], z_input[indices, 1], c=label_colors[label], label=f'Label {label}')\n",
    "\n",
    "    plt.xlabel(\"Latent Dimension 1\")\n",
    "    plt.ylabel(\"Latent Dimension 2\")\n",
    "    plt.title(\"Input Data\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_reconstructed_data(encoder, decoder, x_data, y_data):\n",
    "    _, _, z = encoder.predict(x_data)\n",
    "    reconstructed_data = decoder.predict(z)\n",
    "    _, _, recon_data_lsp = encoder.predict(reconstructed_data)\n",
    "    unique_labels = np.unique(y_data)\n",
    "    label_colors = ['blue', 'red', 'green', 'orange']\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        indices = np.where(y_data == label)[0]\n",
    "        plt.scatter(recon_data_lsp[indices, 0], recon_data_lsp[indices, 1], \n",
    "                    c=label_colors[label], label=f'Label {label}')\n",
    "\n",
    "    plt.xlabel(\"Latent Dimension 1\")\n",
    "    plt.ylabel(\"Latent Dimension 2\")\n",
    "    plt.title(\"Reconstructed Data\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_input_data(encoder, x_train, y_train) \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_reconstructed_data(encoder, decoder, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da681f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import pairwise2\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "def calculate_sequence_similarity(seq1, seq2):\n",
    "    # Convert the sequences to Seq objects\n",
    "    seq1 = Seq(seq1)\n",
    "    seq2 = Seq(seq2)\n",
    "\n",
    "    # Perform global sequence alignment\n",
    "    alignments = pairwise2.align.globalxx(seq1, seq2)\n",
    "\n",
    "    # Get the alignment with the best score (the first one in the list)\n",
    "    best_alignment = alignments[0]\n",
    "    seq1_aligned, seq2_aligned, score, begin, end = best_alignment\n",
    "\n",
    "    # Calculate similarity as a percentage\n",
    "    similarity = (score / max(len(seq1), len(seq2))) * 100\n",
    "\n",
    "    return similarity\n",
    "\n",
    "similarities_train = []\n",
    "similarities_test = []\n",
    "\n",
    "for i in range(50):\n",
    "    row_similarities_train = []\n",
    "    row_similarities_test = []\n",
    "    for j in range(50,100):\n",
    "        sim_train = calculate_sequence_similarity(x_train_seq[i], x_train_seq[j])/100\n",
    "        sim_test = calculate_sequence_similarity(x_test_seq[i], x_test_seq[j])/100\n",
    "        row_similarities_train.append(sim_train)\n",
    "        row_similarities_test.append(sim_test)\n",
    "        \n",
    "    similarities_train.append(row_similarities_train)\n",
    "    similarities_test.append(row_similarities_test)\n",
    "    \n",
    "similarities_train = np.array(similarities_train)\n",
    "similarities_test = np.array(similarities_test)\n",
    "\n",
    "print(similarities_train)\n",
    "print(similarities_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd1d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, encoded_x_train_dist = vae.encoder.predict(x_train)\n",
    "_, _, encoded_x_test_dist = vae.encoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eea0730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x, y):\n",
    "    return np.sqrt(np.sum((x - y)**2))\n",
    "\n",
    "def calculate_distance(latent_representations):\n",
    "    num_peptides = len(latent_representations)\n",
    "    distance_matrix = np.zeros((num_peptides, num_peptides))\n",
    "    \n",
    "    for i in range(num_peptides):\n",
    "        for j in range(num_peptides):\n",
    "            distance_matrix[i, j] = euclidean_distance(latent_representations[i], latent_representations[j])\n",
    "    \n",
    "    return distance_matrix\n",
    "\n",
    "distances_train = []\n",
    "distances_test = []\n",
    "\n",
    "for i in range(50):\n",
    "    row_distances_train = []\n",
    "    row_distances_test = []\n",
    "    for j in range(50,100):\n",
    "        dist_train = euclidean_distance(encoded_x_train_dist[i], encoded_x_train_dist[j])\n",
    "        dist_test = euclidean_distance(encoded_x_train_dist[i], encoded_x_test_dist[j])\n",
    "        row_distances_train.append(dist_train)\n",
    "        row_distances_test.append(dist_test)\n",
    "        \n",
    "    distances_train.append(row_distances_train)\n",
    "    distances_test.append(row_distances_test)\n",
    "        \n",
    "distances_train = np.array(distances_train)\n",
    "distances_test = np.array(distances_test)\n",
    "\n",
    "normalized_distances_train = scaler.fit_transform(distances_train)\n",
    "normalized_distances_test = scaler.fit_transform(distances_test)\n",
    "\n",
    "print(normalized_distances_train)\n",
    "print(normalized_distances_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32282043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr, kendalltau, pearsonr\n",
    "\n",
    "correlation_train, p_value_train = spearmanr(similarities_train.flatten(), distances_train.flatten())\n",
    "correlation_test, p_value_test = spearmanr(similarities_test.flatten(), distances_test.flatten())\n",
    "\n",
    "print(\"Spearman's Correlation:\", correlation_train, correlation_test)\n",
    "print(\"p-value:\", p_value_train, p_value_test)\n",
    "\n",
    "correlation_train, p_value_train = pearsonr(similarities_train.flatten(), distances_train.flatten())\n",
    "correlation_test, p_value_test = pearsonr(similarities_test.flatten(), distances_test.flatten())\n",
    "\n",
    "print(\"Pearson's Correlation:\", correlation_train, correlation_test)\n",
    "print(\"p-value:\", p_value_train, p_value_test)\n",
    "\n",
    "correlation_train, p_value_train = kendalltau(similarities_train.flatten(), distances_train.flatten())\n",
    "correlation_test, p_value_test = kendalltau(similarities_test.flatten(), distances_test.flatten())\n",
    "\n",
    "print(\"Kendall's tau Correlation:\", correlation_train, correlation_test)\n",
    "print(\"p-value:\", p_value_train, p_value_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
